{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "from math import log2\n",
    "from sklearn import datasets,model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Node here.\n",
    "class TreeNode:\n",
    "    def __init__(self,data,output):\n",
    "        #data is the best feature upon which split was made,output is the y_prediction.\n",
    "        self.data=data\n",
    "        self.output=output\n",
    "        #children is dictionary with key as the label of feature upon which split was made, and value is the node formed.\n",
    "        self.children={}\n",
    "    #Addition of child node to the parent node.\n",
    "    def add_child(self,labels,child):\n",
    "        self.children[labels]=child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root=None\n",
    "        \n",
    "        \n",
    "    #This function will calculate entropy of a single node, by finding probability and multiplying it with log\n",
    "    def entropy(self,y_train):\n",
    "        #evaluating length of y_train\n",
    "        length_y=len(y_train)\n",
    "        ent=0\n",
    "        #unique values present in y_train\n",
    "        unique_output=set(y_train)\n",
    "        for labels in unique_output:\n",
    "            #will return boolean, and then .sum() will add all the true(i.e 1)\n",
    "            p=(y_train==labels).sum()/length_y\n",
    "            ent+=p*log2(p)\n",
    "        return -1*ent\n",
    "    \n",
    "    \n",
    "    \n",
    "    def gain_ratio(self,x_train,y_train,selected_feature):\n",
    "        #calculating the initial entropy of a node before split\n",
    "        original_entropy=self.entropy(y_train)\n",
    "        length=len(y_train)\n",
    "        #selecting column with selected feature\n",
    "        column_with_selected_feature=(x_train[:,selected_feature])\n",
    "        # unique value that the column can take\n",
    "        unique_labels=set(column_with_selected_feature)\n",
    "        split_info=0\n",
    "        entropy_f=0\n",
    "        #now finding entropy and split ratio for all the nodes, formed after split\n",
    "        for labels in unique_labels:\n",
    "            #selecting only those y that has selected feature = labels\n",
    "            y=y_train[column_with_selected_feature==labels]\n",
    "            #applying split ratio formula and adding it.\n",
    "            split_info+=(len(y)/length)*log2(len(y)/length)*-1\n",
    "            #calculating weighted entropy and adding\n",
    "            entropy_f+=self.entropy(y)*(len(y)/length)\n",
    "        #if split_info is 0,split is not allowed,therfore return gain_ratio as -1\n",
    "        if split_info==0:\n",
    "            return -1\n",
    "        return (original_entropy-entropy_f)/split_info\n",
    "\n",
    "    \n",
    "    \n",
    "    def decision_tree(self,x_train,y_train,features,level):\n",
    "        unique_y=set(y_train)\n",
    "        #Pure class\n",
    "        if len(unique_y)==1:\n",
    "            print(\"level:\",level)\n",
    "            for i in unique_y:\n",
    "                print(\"count of\",i,\":\",(y_train==i).sum())\n",
    "                #Finding the y_prediction,here it will be the single label that is present.\n",
    "                output=i\n",
    "            print(\"Entropy:\",abs(self.entropy(y_train)))\n",
    "            print(\"Reached Leaf Node\")\n",
    "            print()\n",
    "            #As there is no further split, so feature selected will be None \n",
    "            return TreeNode(None,output)\n",
    "        #features are over\n",
    "        elif len(features)==0:\n",
    "            max_freq=0\n",
    "            print(\"level:\",level)\n",
    "            for i in unique_y:\n",
    "                print(\"count of\",i,\":\",(y_train==i).sum())\n",
    "                #Finding the class with majority, as that will be the y_prediction\n",
    "                if max_freq<(y_train==i).sum():\n",
    "                    output=i\n",
    "                    max_freq=(y_train==i).sum()\n",
    "            print(\"Entropy:\",abs(self.entropy(y_train)))\n",
    "            print(\"Reached Leaf Node\")\n",
    "            print()\n",
    "            #As there is no further split, so feature selected will be None \n",
    "            return TreeNode(None,output)\n",
    "        else:\n",
    "            #finding out the best split\n",
    "            max_gain=0\n",
    "            selected_feature=None\n",
    "            #splitting on every feature to decide best gain_ratio\n",
    "            for f in features:\n",
    "                gain=self.gain_ratio(x_train,y_train,f)\n",
    "                if(max_gain<gain):\n",
    "                    max_gain=gain\n",
    "                    selected_feature=f\n",
    "            #max_gain will be 0 in case split info is 0,or information gain is 0,in both this case there will be no split\n",
    "            if max_gain==0:\n",
    "                return TreeNode(None,None)\n",
    "            print(\"level:\",level)\n",
    "            max_freq=0\n",
    "            for i in unique_y:\n",
    "                print(\"count of\",i,\":\",(y_train==i).sum())\n",
    "                if max_freq<(y_train==i).sum():\n",
    "                    output=i\n",
    "                    max_freq=(y_train==i).sum()\n",
    "            #Made a node,data is here selected_feature and output is what we found above.\n",
    "            current_Node=TreeNode(selected_feature,output)\n",
    "            inde=features.index(selected_feature)\n",
    "            #removing the selected feature from list\n",
    "            features.remove(selected_feature)\n",
    "            column_with_selected_feature=x_train[:,selected_feature]\n",
    "            unique_labels_in_selected_feature=set(column_with_selected_feature)\n",
    "            print(\"Entropy:\",abs(self.entropy(y_train)))\n",
    "            print(\"Splitting on feature\",selected_feature,\"with gain-ratio\",max_gain)\n",
    "            print()\n",
    "            #now will call recursion for all whose values which the selected feature can take\n",
    "            for labels in unique_labels_in_selected_feature:\n",
    "\n",
    "                x_new=x_train[column_with_selected_feature==labels]\n",
    "                y_new=y_train[column_with_selected_feature==labels]\n",
    "                child_node=self.decision_tree(x_new,y_new,features,level+1)\n",
    "                #adding the child_node to the current node.\n",
    "                current_Node.add_child(labels,child_node)\n",
    "            #Adding the removed feature again\n",
    "            features.insert(inde,selected_feature)\n",
    "            return current_Node\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #converting continuous data to discrete data by mean method\n",
    "    def labelled_data(self,x_train):\n",
    "        mean=x_train.mean()\n",
    "        first_mean=0.5*mean\n",
    "        second_mean=1.5*mean\n",
    "        for i in range(len(x_train)):\n",
    "            if x_train[i]<first_mean:\n",
    "                x_train[i]=0\n",
    "            elif x_train[i]<mean:\n",
    "                x_train[i]=1\n",
    "            elif x_train[i]<second_mean:\n",
    "                x_train[i]=2\n",
    "            else:\n",
    "                x_train[i]=3\n",
    "        return x_train\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        #converting continuous data into discrete,column wise\n",
    "        for col in range(x_train.shape[1]):\n",
    "            x_train[:,col]=self.labelled_data(x_train[:,col])\n",
    "        features=[i for i in range(x_train.shape[1])]\n",
    "        #storing the root, and will access rest as done in TREE\n",
    "        self.root=self.decision_tree(x_train,y_train,features,0)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict_for_single(self,X,node):\n",
    "        #will reach leaf node and predict output\n",
    "        if len(node.children)==0:\n",
    "            return node.output\n",
    "        val=X[node.data]\n",
    "        #if the label is not present in children, there will be no further split and declare the output there itself.\n",
    "        if val not in node.children:\n",
    "            return node.output\n",
    "        #recursively finding output\n",
    "        return self.predict_for_single(X,node.children[val])\n",
    "    \n",
    "    def predict(self,X):\n",
    "        length=X.shape[0]\n",
    "        y_pred=[]\n",
    "        #will predict output for each testing point and will append it in y_pred\n",
    "        for i in range(length):\n",
    "            y=self.predict_for_single(X[i,:],self.root)\n",
    "            y_pred.append(y)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        Y_pred=self.predict(X)\n",
    "        #will just find correct y divide by total y\n",
    "        correct_output=(Y_pred==Y).sum()\n",
    "        return correct_output/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 0\n",
      "count of 0 : 1\n",
      "count of 1 : 3\n",
      "Entropy: 0.8112781244591328\n",
      "Splitting on feature 0 with gain-ratio 0.31127812445913283\n",
      "\n",
      "level: 1\n",
      "count of 0 : 1\n",
      "count of 1 : 1\n",
      "Entropy: 1.0\n",
      "Splitting on feature 1 with gain-ratio 1.0\n",
      "\n",
      "level: 2\n",
      "count of 0 : 1\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 2\n",
      "count of 1 : 1\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 1\n",
      "count of 1 : 2\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "score: 1.0\n"
     ]
    }
   ],
   "source": [
    "iris=datasets.load_iris()\n",
    "x = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([0,\n",
    "              1,\n",
    "              1,\n",
    "              1])\n",
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(x,y)\n",
    "y_pred=clf.predict(x)\n",
    "print(\"score:\",clf.score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 0\n",
      "count of 0 : 50\n",
      "count of 1 : 50\n",
      "count of 2 : 50\n",
      "Entropy: 1.584962500721156\n",
      "Splitting on feature 3 with gain-ratio 0.7350016280496156\n",
      "\n",
      "level: 1\n",
      "count of 0 : 49\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 1\n",
      "count of 0 : 1\n",
      "count of 1 : 10\n",
      "Entropy: 0.4394969869215134\n",
      "Splitting on feature 1 with gain-ratio 1.0\n",
      "\n",
      "level: 2\n",
      "count of 1 : 10\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 2\n",
      "count of 0 : 1\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 1\n",
      "count of 1 : 39\n",
      "count of 2 : 5\n",
      "Entropy: 0.5107878229540133\n",
      "Splitting on feature 2 with gain-ratio 0.2488471906913506\n",
      "\n",
      "level: 2\n",
      "count of 1 : 1\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 2\n",
      "count of 1 : 38\n",
      "count of 2 : 4\n",
      "Entropy: 0.4537163391869448\n",
      "Splitting on feature 1 with gain-ratio 0.04070432026142338\n",
      "\n",
      "level: 3\n",
      "count of 1 : 31\n",
      "count of 2 : 4\n",
      "Entropy: 0.512709142030877\n",
      "Splitting on feature 0 with gain-ratio 0.012981006561098145\n",
      "\n",
      "level: 4\n",
      "count of 1 : 14\n",
      "count of 2 : 1\n",
      "Entropy: 0.35335933502142136\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 4\n",
      "count of 1 : 17\n",
      "count of 2 : 3\n",
      "Entropy: 0.6098403047164004\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 3\n",
      "count of 1 : 7\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 2\n",
      "count of 2 : 1\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 1\n",
      "count of 1 : 1\n",
      "count of 2 : 45\n",
      "Entropy: 0.15109697051711368\n",
      "Splitting on feature 1 with gain-ratio 0.031037861792700953\n",
      "\n",
      "level: 2\n",
      "count of 2 : 28\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "level: 2\n",
      "count of 1 : 1\n",
      "count of 2 : 17\n",
      "Entropy: 0.3095434291503252\n",
      "Splitting on feature 2 with gain-ratio 0.057914261762502306\n",
      "\n",
      "level: 3\n",
      "count of 2 : 9\n",
      "Entropy: 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "score: 0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "#Trying on iris dataset\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(X,Y)\n",
    "y_pred=clf.predict(X)\n",
    "print(\"score:\",clf.score(X,Y))\n",
    "#Score is not 1 becuase we have done by converting continuous into categorical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
